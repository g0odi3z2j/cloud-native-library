---
linktitle: 第 9 章：应用 WebAssembly——TensorFlow.js
summary: WebAssembly 表。
weight: 10
icon: book-reader
icon_pack: fas
draft: false
title: 应用 WebAssembly：TensorFlow.js
date: '2023-01-26T00:00:00+08:00'
type: book # Do not modify
---

> 现在，世界并不只是按照一个鼓的节拍前进，对你来说可能是正确的，对某些人来说可能并不正确。
>
> ——Diff'rent Strokes 的主题（译者注：中文名《小淘气》，美国情景喜剧，于 1978 年开播）

这是 "应用 WebAssembly" 的第一个章节，我强调了该技术的潜在使用案例。正如你在本书中所看到的，没有单一的用途。相反，设计者已经精心设计了一个平台，其范围越来越大，几乎涉及到软件行业的方方面面。因此，请跟随我的脚步。我不会教授新功能本身。相反，我希望能帮助你了解我们的行业是如何快速变化的，以及 WebAssembly 将如何协助。

首先，我希望你停下来想一想，关于编程语言和机器学习。你想到的第一种编程语言是什么？你的答案可能很有可能不是 JavaScript。为什么会是这样呢？机器学习是一种令人难以置信的以性能为导向的活动，如今有巨大的计算工作负荷。

像 Python 这样的语言与机器学习的联系要比 JavaScript 强得多。如果我们说实话，这也有点牵强。Python 本身就是一种可怕的语言。然而，它在可读性、灵活的编程风格（函数式、面向对象、过程式）以及广泛的算法、可视化和数据处理功能方面取得了良好的平衡。如果你能让它运行得更快，那么它就能满足大量的需求。NumPy、本地库、云计算环境等可以给它提供计算能力，使训练过程得以实现。
但大多数组织并不运行 Python，所以你有可能需要将模型序列化为其他格式，以便将其加载到 C/C++、C#、Java 或 JavaScript 应用程序中。这一点正变得越来越容易，因为有开放的格式，如 [开放神经网络交换（ONNX）](https://onnx.ai/)，但许多框架，如 TensorFlow 自然支持这种形式的混合使用。

第一步是训练模型。第二步是推理。直到几年前，TensorFlow 要求你在 Python 中进行训练，但你可以将模型保存到磁盘中并将其加载到其他环境中。如今，你可以选择用 TensorFlow 在 Python、JavaScript 或 Swift 中训练。

我们将讨论 TensorFlow.js，它被设计为一种在 JavaScript 环境下在浏览器内提供机器学习能力的方式。将 JavaScript 用于机器学习（尤其是在浏览器中），比你想象的更有意义。然而，为了理解机器学习与 JavaScript 和 WebAssembly 之间的关系，我们必须首先快速讨论一下硬件以及它是如何改变我们的行业的。

## 硬件设施

典型的程序员认为他们的工作是编写在包含中央处理单元（CPU）、主存储器、存储系统、显示器和一些输入设备的计算机上运行的软件。虽然这仍然涵盖了大量的软件开发，但我们有一个比大多数人意识到的更丰富的计算运行机制的生态系统。计算机、平板电脑、游戏机、网络设备、智能手机、手表、嵌入式系统、物联网（IoT）设备和单片机（SOC）提供了一个执行软件的硬件系统的集合体。

著名的 Herb Sutter 写了一篇出色的文章，名为 "[欢迎来到丛林](https://herbsutter.com/welcome-to-the-jungle/)"，值得你一读 [^1]。在文中，他强调了过去几十年来硬件发展对软件业的影响。大约 30 年来，摩尔定律将更高的密度转化为更快的芯片 [^2]。如果你的软件很慢，你只需等待 18 到 24 个月，它就会变得更快。

一旦我们不再能够制造更快、更复杂的芯片，我们就利用额外的密度来制造更简单的芯片。多核系统成为常态。与之前的 "免费午餐" 时期不同，现在的开发者必须编写疯狂的并发代码，以从额外的处理能力中获益，而不是每个问题都能很好地适应这种模式。另外，这也是很棘手的代码，很容易出错。这在很大程度上把我们推向了功能化的编程语言和不可变的数据结构。

其他发展包括云计算的出现，以满足弹性，边缘计算在地理上分布，以实现低延迟，以及异质计算环境。这包括图形处理单元（GPU）、现场可编程门阵列（FPGA）和特定应用集成电路（ASIC）。

所有这一切都与计算需要时间和电力的想法联系在一起。一个成功的 IT 战略的很大一部分将是关于最小化时间成本、电力成本和延迟成本。这将影响事物的运行。将大量的数据推送到云端进行弹性的、突发性的训练是有意义的。但是，如果这导致了大型模型，由于规模问题，这些模型就不太容易分布到桌面 / 移动体验中。同时，我们也不希望互联网传感器和汽车制动器在关键时刻进行云调用。

我们正面临着数据的绝对爆炸，这将使所有这些变得更加关键。为了在合理的时间内处理数据，我们需要访问硬件加速。这使数学平行化，所以它是可操作的。正是在这个世界上，我们终于可以在浏览器中探索基于 JavaScript 的机器学习的概念。

## Playground

[TensorFlow Playground](http://playground.tensorflow.org/) 是一个实验环境，供非专业人士通过直接操作获得关于神经网络如何工作的直观感受 [^3]。我们的想法是，这些模型越来越多地驱动着具有现实意义的系统，但理解它们所需的背景超出了非学术研究人员的能力范围。通过使用可视化表示，非技术用户能够获得一种直观感觉。

问题是，JavaScript 环境是单线程的，而且对于训练神经网络所需的那种数学运算来说，数字支持不是很好。Playground 环境允许你通过简单的用户界面操作来改变训练过程中的所有参数。虽然改变这些参数的实验很容易也很有趣，但它们会对结果的质量产生重大影响。为了快速和实时地看到效果，运行时需要重新计算代码的重要部分；否则，直接操作的全部意义就会丧失。幸运的是，它们能够通过依靠浏览器中的 WebGL 来获得足够的性能，使这一切顺利进行。我们稍后会进一步讨论这个想法。

## TensorFlow.js

Playground 的成功让人们认识到将深度学习系统引入浏览器的必要性和可能性。想在手机或平板电脑等潜在的低端设备上运行这种计算密集型系统，可能会让你觉得有点奇怪。即使只是在强大的台式机上的浏览器中运行，也是一个稍微奇怪的概念。不过，这个想法还是有不少好处的。

点击一个链接并下载一个基于深度学习的应用程序的体验，要比安装潜在的大量所需库容易得多。加载一个网页是零安装。如果这么容易，分享研究和实际应用就更容易了。这拓宽了深度学习研究人员之间的互动潜力，并使采用这些能力的应用程序更容易针对终端用户。

考虑到 JavaScript 的普及，要求 Web 开发者学习 Python 以进行机器学习，有点强人所难了。有大量的开源 JavaScript 代码用于构建各种形式的软件系统。能够利用这一切，对于机器学习系统的开发者来说，将是另一个好处。

我们的手机和平板电脑已经迅速成为远远超过移动便携式电子设备的东西。它们已经成为诊断工具，我们银行系统的一部分，识别方法，等等。人们正在编写新的软件，通过分析他们的摄像头视频来检测痴呆症的发病或中风的存在。无论是对用户体验的质量，还是对诊断结果的隐私法合规性，能够将应用程序推送到设备上，将比试图从设备上获取数据更少地受到监管负担的困扰。

最后，这些下载的应用程序可能运行的许多设备都有强大而复杂的 GPU 可供使用。不仅有可能在浏览器中做深度学习系统，而且可能真的表现良好。

那么，我们为什么要讨论这一切呢？

TensorFlow.js 框架的设计是优雅的，其具有干净的 API，可以在广泛的设备上工作。设计者没有把实现限制在到处都有的东西上，而是选择创建一个可插拔的后端，以覆盖最广泛的系统。

后台的基本版本是一个基于 CPU 的 JavaScript 实现，可以在任何地方运行。所有的代码都直接在 CPU 上执行，不需要借助于优化指令集，如高级向量扩展（AVX）[^4]。它不是在所有地方都表现高效，但它基本上可以在任何地方运行。你可能有更好的选择，但这是一个体面的后备选择。

下一个主要的后端是由 WebGL 加速的。没有直接支持 从 JavaScript 访问 GPU，但通过 WebGL，几乎所有的浏览器都支持。通过 3D 图形，开发人员可以在 GPU 上执行计算，并将结果写入 texture 中。这产生了一个快速和方便的实现，在几乎所有主要的现代浏览器上运行良好。这与前面提到的 TensorFlow Playground 应用的情况类似。

{{<callout note 说明>}}

这并不像它听起来那么奇怪的情况。在 CUDA、OpenCL、Metal 和 Vulkan 等 GPU 计算库出现之前，研究人员会用 OpenGL 来获得相对便宜的计算。这些专门允许使用 GPU 进行任意计算的 API 是加速我们现在享受的机器学习和深度学习系统的部分原因。

{{</callout>}}

有深度学习需求的服务器可以在 Node.js 中实现，所以下一个后端被设计成在比浏览器更自由的环境中运行。在 Node.js 中运行的应用程序可以从文件系统中读取和写入，加载和使用本地库，并直接与正常的本地 TensorFlow 库进行通信。这些可以利用多核系统、GPU 或其他硬件加速设备，如谷歌的张量处理单元（TPU）。
你可以看到 API 设计的基本结构，如图 9-1 所示。

![图 9-1.TensorFlow.js 的分层 API 和后端](../images/f9-1.png)

这里有相当多的东西需要解读。首先，低级别的操作者是通过上面显示的 Ops API 暴露的。需要深入了解细节的开发者在这个层面上，可以选择进入 API。然而，在这之上，是一个受 Keras 启发的 Layers API，其设计考虑到了开发者体验。这是一个简单的 API，用于使用简化的堆叠层方法定义复杂的神经网络架构。它隐藏了许多细节，但仍然通过一个相对简单的机制暴露了强大的行为。

无论客户端代码使用哪一层的 API 进入 TensorFlow.js，针对这些 API 编写的应用程序将在后端覆盖的所有环境中可移植。这包括在浏览器中，在浏览器外，有硬设备加速和没有硬设备加速。这已经是很了不起的设计成就了，但是当代码能够利用硬件加速的时候，它在不同的环境中也会运行得非常好。正如我们所看到的，这可以以各种形式出现，但我们不是在针对最小公分母的环境进行编写，应用程序代码不需要被一堆功能所束缚 —— 测试代码要看环境提供什么。

使用 MobileNet 进行单次推理的结果，在一百次运行中的平均值如表 9-1 所示 [^5]。这个数据取自介绍 TensorFlow.js 的论文 [TensorFlow.js: Machine Learning for the Web and Beyond](https://arxiv.org/pdf/1901.05350.pdf)[^6]。

| Backend                 | Time (ms) | Speedup |
| ----------------------- | -------- | ------- |
| CPU JavaScript          | 3426     | 1x      |
| WebGL (Intel Iris Pro)  | 49       | 71x     |
| WebGL (GTX 1080)        | 5        | 685x    |
| Node.js CPU w/ AVX2     | 87       | 39x     |
| Node.js CUDA (GTX 1080) | 3        | 1105x   |

作为一种趋势，结果并不令人惊讶，但具体细节却令人惊讶。在没有硬件加速的情况下，在原始的 JavaScript 后端居然要运行 3.5 秒。只要把后端换成一个通过 WebGL 支持优化的后端，即使是一个非常普通的集成 GPU，在浏览器中运行也会有 71 倍的速度提升。在拥有更强大的 GPU 的台式机上，我们实现了近 700 倍的速度提升。一个服务器应用程序在没有 GPU 的情况下运行，但在具有优化 AVX 指令的 CPU 的帮助下，速度提高了近 40 倍。同样的环境，在更强大的 GPU 支持下，产生了令人瞠目结舌的 1000 倍以上的速度提升。请记住，应用程序没有任何变化，只是运行的环境发生了变化。

这是我更大的观点。从 "免费午餐" 时期到多核时期的转变，迫使开发人员不得不在编程方面做出重大改变，以利用这些额外资源。如果我们不针对自定义加速、多个 CPU 的存在、云主机环境以及在浏览器中运行进行调整，那么我们的深度学习应用程序将成为一个无法维护的代码的无底洞。相反，我们看到的是一个利用硬件加速的选项。这将我们引向我们要考虑的最后一个后端。

## WebAssembly 后端

在 TensorFlow.js 发布后不久，该团队发布了一个新的 WebAssembly 的后端写法。考虑到我们刚才讨论的覆盖面，他们觉得有必要这样做，这可能会让你感到惊讶。然而，现在我希望你意识到，这可能并不意味着从头开始。显然，有一些工作要做，但相当多的工作是依靠现有的代码来完成的。特别是，XNNPack 库被扩展以支持 WebAssembly 的构建 [^7]。

这就扩展了我们的后端集合，包括如图 9-2 所示的后端。 

![图 9-2. TensorFlow.js 的 WebAssembly 后端](../images/f9-2.png)

这个例子强调了 WebAssembly 的一个突出用例。它不是要取代 JavaScript，至少不是在所有情况下。它是关于扩展浏览器中的可能，而不必等待浏览器供应商的共识。WebGPU [^8] 或类似的东西正在通过标准过程，但这可能需要几年时间。一个用 C++ 编写的、被设计成可以在许多平台上进行优化和移植的现有库，现在就可以为浏览器带来很大一部分的能力，而不需要等待。

WebAssembly 后端扩大了计算运行时，包括没有强大 GPU 的旧设备。代码可以被优化，以便在广泛的平台上仍然运行良好。此外，WebAssembly 的设计者正在采纳我们将在后面的章节中讨论采用两个高级特性，即使用单指令多数据（SIMD）并行化和使用多线程 [^9]。

在 图 9-3 中，我们对 MobileNet 的使用与之前类似。很明显，WebAssembly 后端本身的性能不如 WebGL 支持的后端好。我们鼓励你使用 Wasm 后端而不是普通的 JavaScript 后端，因为它在任何地方都会表现得更好。根据模型的大小，Wasm 后端比 WebGL 后端更有意义，因为在 WebGL 执行方面有一些固定的成本。请注意，当线程和 SIMD 并行化被添加到 Wasm 组合中时，对于支持它们的平台来说，性能会有巨大的改善。

![图 9-3. TensorFlow.js 后端在不同平台上的相对性能（来源：https://github.com/tensorflow/tfjs/tree/master/tfjs-backend-wasm）](../images/f9-3.png)

从图 9-4 中我们看到一个参数数量较少的不同模型的结果。在这种情况下，Wasm 的后端更有意义。请注意，Pixel 4 的性能相当，而 Linux 和 Mac 笔记本的性能则大幅提高。同样，线程和 SIMD 的使用极大地改善了基于 WebGL 的后端性能。

![图 9-4. TensorFlow.js 后端在不同平台上的相对性能（来源：https://github.com/tensorflow/tfjs/tree/master/tfjs-backend-wasm）](../images/f9-4.png)

至少在目前，WebAssembly 并不总是在任何情况下都是最快的解决方案。它是一个相对较新的平台，在所有的潜在的优化推出之前，还需要一段时间。然而，鉴于 WebAssembly 的年轻，它已经改变了在广泛的计算环境中的游戏规则。

本章的主要收获是，WebAssembly 及其相关技术可以为我们提供更多的选择，而且在许多情况下是更好的选择，以便将高性能的软件系统部署到不同的硬件和软件系统中。随着时间的推移，我们将看到许多其他用途，但现在是时候扩大我们在语言和 WebAssembly 方面的选择了。

## 注释

[^1]:  在这篇文章中他引用了自己的另一篇文章《免费午餐结束了》，也值得去看一看。
[^2]: [摩尔定律](https://en.wikipedia.org/wiki/Moore%27s_law) 是一个关于硅芯片密度的制造趋势的著名观察。
[^3]: [直接操作](https://en.wikipedia.org/wiki/Direct_manipulation_interface) 是一种用户界面的风格，在这种风格中，用户拥有允许安全、探索性地操作应用程序及其数据的控件。
[^4]: [AVX/AVX2](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions) 是对英特尔 x86 指令集架构（ISA）的扩展，为我们在这里使用的那种数学提供并行优化。
[^5]: [MobileNet 系列模型](https://arxiv.org/pdf/1704.04861.pdf) 是为高效的移动和嵌入式视觉应用而设计的。
[^6]: 该文件是一个很好的读物，值得你花时间阅读。
[^7]: XNNPack 可在 在 [GitHub](https://github.com/google/XNNPACK) 上获取，尽管它并不打算被深度学习研究人员直接使用。
[^8]: 这个 [新兴标准](https://en.wikipedia.org/wiki/WebGPU) 将以跨平台的方式将 GPU 支持直接暴露给浏览器。
[^9]: [SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data) 是一种数据并行处理，在数据的不同部分对多个计算元素执行相同的指令。
